<!DOCTYPE html>
<html>

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47879725-7"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-47879725-7');
    </script>

    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
    <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/icons/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/icons/ms-icon-144x144.png">
    <meta http-equiv="cache-control" content="max-age=0" />
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="expires" content="0" />
    <meta http-equiv="expires" content="Tue, 01 Jan 1980 1:00:00 GMT" />
    <meta http-equiv="pragma" content="no-cache" />
    <title>Masq Blog</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="../css/blog.min.css">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>

<body>
    <div class="container">
        <h1>
            <h1>No, AI Isn't Taking Over the World <small></small></h1>
        </h1>
        <a href="../" style="
        font-weight: 800;
        text-decoration: unset;
    ">&lt; Back to Blog</a>
        <h6>Tagged: ['op-ed', 'scholastic']
            <span>12/15/16 || 1:43AM</span></h6>
        <p>
            <p></p>
            <p>Robots are becoming increasingly involved in our lives--according to billionaire Elon Musk, too involved. Musk
                and his friends are investing billions into researching the dangers of artificial intelligence and illuminating
                the deep ethical consequences of robotics, raising an important question: should robots have ethics?</p>
            <p> </p>
            <p>All debates of artificial intelligence begin with an analysis of human intelligence. The process by which we
                think has been praised as the hidden, complex network of the brain; building a machine with these capabilities,
                surely, is impossible, preposterous even--right?</p>
            <p> </p>
            <p>Well, perhaps.</p>
            <p> </p>
            <p>Simple choices and moral decisions are objectively made by perceiving our surroundings, fitting them to moral
                guidelines, and deciding on an appropriate course of action. A robot equipped with the right variety of sensors
                can also make its own observations, analyze these with moral codes, and choose a course of action, thinking
                and learning through techniques such as "neural networking". Neural networking composes of layers of nodes
                and interconnected weighting systems to emulate the human brain--given enough data, it can <i>theoretically</i>                predict anything.</p>
            <p></p>
            <p>Complex thought patterns and abstraction, on the other hand, are still a human-exclusive trait. Fortunately,
                ethical agency can be achieved without higher level thinking.</p>
            <p> </p>
            <p>Knowing this enables us to progress past the plausibility of "can" to the dilemma of "should." Robots are increasingly
                integrating into our lifestyle, from the infamous vacuum Roomba to the nursing-caretaker Robear; the future's
                machine-integrated society will often come across ethically sensitive situations. Should a caretaker prioritize
                an urgent call or the owner's privacy? Should the autonomous cars of Tesla and Google swerve to save the
                driver while endangering the bystanders, or sacrifice the owner to minimize casualties? These delicate situations
                make it imperative for these automatons to hold ethical agencies--codes of ethics by which to abide by and
                make decisions upon.</p>
            <p> </p>
            <p>Companionship robots pose the most precarious situation. Accounting for privacy concerns, these robots need an
                ethical code; according to ethical researchers such as Dr. Mattheus Scheutz of Tufts University, the actions
                of an oblivious automaton may be "insensitive to social norms based on consideration." Scheutz found that
                as the machine takes actions towards its directive (i.e. cleaning the house) with no regard to the owner,
                <b>it makes ethical mistakes</b> such as intruding during an emotionally sensitive time.</p>
            <p></p>
            <p>Another fault of household robotics lies in emotional connection. Studies show that most modern caretaking robots
                have low ratings, with an average of 3.4 out of 10 due to an inability to form long-lasting and meaningful
                connections with the owner--these robots are simply incapable of the level of sensitivity necessary for social
                interaction. Dr. Gordon Briggs, another scholar at Tufts' Human-Robot Interaction Laboratory, has developed
                studies showing that as household robots perform tasks, their owner grows more and more appreciative of their
                servitude. This causes the owner to develop a sort of emotional attachment, but these robots are incapable
                of ethical reaction and therefore appear "cold-hearted and unemotional."</p>
            <p> </p>
            <p>Ethical dilemmas are not limited to the field of caretaking. Another field, assistive robotics, raises the same
                concerns, albeit in a more tangible and serious manner. Scholars such as Dr. Wendell Wallach of Yale University,
                a celebrity of sorts in the field of robot ethics, have criticized the lack of ethical agency present in
                modern medical systems, particularly that of APACHE. In modern hospitals, APACHE systems control intensive
                care units to house those in critical conditions. APACHE enjoys the cruel pleasure of disastrous autonomy,
                the freedom to choose any course of action with no programmed ethics. Without a moral agency, it may pursue
                an unethical yet logical directive, posing a lethal risk to its constituents. In therapeutic robotics, the
                automaton may push its patient forward without any care for distress. Without the proper ethics to care for
                its patient, an assistive robot is liable to cause emotional and physical harm.</p>
            <p></p>
            <p>However, with appropriate ethical guidelines, these machines will be capable of implicit learning and ethical
                reaction. </p>
            <p>With a moral agency, the robot can respect the owner's boundaries and identify intrusions of privacy. </p>
            <p>With emotional sensitivity, the robot can respond to its owner in an ethically appropriate manner. </p>
            <p>With a code of ethics, medical and assistive robotics can avoid unethical directives. </p>
            <p></p>
            <p>So, what's stopping us from pursuing these ethics?</p>
            <p> </p>
            <p>The same fear-gouging demagogues such as Elon Musk who raises the issue of ethics also raise science-fiction
                scenarios of robots growing the ability to learn from themselves, causing an eventual robot rebellion. This
                frightens people, and develops a phenomenon known as the "uncanny valley." Scheutz and Wallach have both
                found in their independent studies that robots given ethical agencies often approach a human-like existence,
                possibly passing the Turing test depicted in science fiction such as <i>Ex Machina</i>. Unfortunately, while
                in <i>Ex Machina</i> the researchers are thrilled by the capabilities of deep learning, in reality most respondents
                are too afraid of autonomy and respond negatively, causing a dip in approval known as the uncanny valley.
                In short, as machines grow very humanlike, their approval increases steadily until they are <i>almost</i>                human; at this point, their approval drops dramatically and only increases as they breach human likeness
                itself.</p>
            <p></p>
            <p>Therefore, the only obstacle to ethical progress in the field of robotics is humankindâ€”we fear the impossible
                too greatly to permit the possible. Only as we become less afraid of the fictional pitfalls of robotics will
                we be able to address the real concern of ethics.</p>
        </p><br><br><br><br><br>
    </div>
</body>

</html>